{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tenencia del producto préstamo en el DataSet de Banco Checo  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a intentar extraer del datset generado que variables son las más relevantes para que una cuenta (account) tenga un péstamo (loan) y ver si de esta forma podemos generar un customer journey para conseguir que un cliente contrate un préstamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.core.multiarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para garantizar la replicabilidad del análisis\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos que hemos procesado en R\n",
    "df_original= pd.read_csv(\"C://Master Data Science/Master en Data Science/TFM/Transacciones de Banco Checo/DFTenenciaProductos2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comenzamos a revisar que el DF se haya importado correctamente\n",
    "df_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a analizar si hay missings al cargar los datos a Python\n",
    "df_original.columns[df_original.isnull().sum()!=0]\n",
    "#Vemos que las variables con missings provienen de variables que ya tenían esos missings en el DataFrame generado con R,\n",
    "#ya que el disponent (autorizado), los préstamos y las tarjetas no son productos que tengan asociados todas las cuentas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Vemos como se han importado las variables del DataFrame de R al DataFrame que vamos a utilizar en Python \n",
    "print(df_original.iloc[:,0:32].dtypes)\n",
    "print(df_original.iloc[:,31:61].dtypes)\n",
    "print(df_original.iloc[:,60:70].dtypes)\n",
    "#Observamos que las variables de factor y de fecha han modificado su tipo de variable, \n",
    "#por lo que tendremos que trabajar con ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Transformamos a formato fecha las variables que originalmente eran fecha en R\n",
    "df_original[[\"Date_Account\",\"birth_owner\", \"birth_disponent\", \"Date_Loan\", \"owner_card_date\"]]=df_original[[\"Date_Account\",\"birth_owner\", \"birth_disponent\", \"Date_Loan\", \"owner_card_date\"]].apply(pd.to_datetime)\n",
    "from datetime import datetime\n",
    "df_original['Date_Account']=df_original['Date_Account'].apply(datetime.toordinal)\n",
    "df_original['birth_owner']=df_original['birth_owner'].apply(datetime.toordinal)\n",
    "df_original['birth_disponent']=df_original['birth_disponent'].apply(datetime.toordinal)\n",
    "df_original['Date_Loan']=df_original['Date_Loan'].apply(datetime.toordinal)\n",
    "df_original['owner_card_date']=df_original['owner_card_date'].apply(datetime.toordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De todas las variables de las que disponemos, vamos a seleccionar las que vamos a utilizar en este ejercicio:\n",
    "\n",
    "#La  variable que vamos a predecir va a ser \"account_loan_bin\" y por tanto la denominaremos \"y\"\n",
    "\n",
    "#Para este ejercicio no consideramos las variables que hemos obtenido en el mismo fichero que la variable a predecir (loan.csv):\n",
    "#loan_id, Date_Loan, Amount_Loan, Duration_Loan, Payments_Loan, status, Status_Loan.\n",
    "\n",
    "#Otras variables que no consideramos: account_id, client_id_owner, client_id_disponent, district ID y unnamed:0 porque \n",
    "#son ID's descriptivas sin información para utilizar\n",
    "\n",
    "#La variable disponent_card_type no aporta información\n",
    "\n",
    "#Otras variables que no consideramos:district_name, region (nombre). Utilizamos el resto de la información de variables \n",
    "#del fichero district.csv\n",
    "\n",
    "#Haciendo referencia a district.csv , la información de crimes_95, crimes_96 y entrepreneurs la vamos a utilizar como ratio,\n",
    "#ya que como veremos a continuación, de esta forma obtendremos menos impacto por un distrito con mucha población (Praga) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Las variables crimes_95, crimes_96 y entrepreneurs vamos a utilizarlas en formato ratio, tal y como calculamos en R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_original['crimes_95'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_original['crimes_95_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_original['crimes_96'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_original['crimes_96_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_original['entrepreneurs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_original['entrepreneurs_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un DataFrame con las variables que vamos a considerar numéricas\n",
    "df_num=df_original[['Date_Account','birth_owner', 'birth_disponent','owner_card_date','Ord_Insurance', 'Ord_Insurance_amount',\n",
    "                    'Ord_Household_Payment','Ord_Household_Payment_amount', 'Ord_Loan_Payment', 'Ord_Leasing',\n",
    "                    'Ord_Empty', 'Ord_Empty_amount', 'num_inhabitants', 'municip < 499', 'municip 500-1999',\n",
    "                    'municip 2000-9999', 'municip > 10000', 'num_cities', 'avg_salary',  \n",
    "                    'Num_Type_Credit', 'Num_Type_VYBER', 'Num_Type_Withdrawal', 'Num_Op_Null', 'Num_Op_Remittances',\n",
    "                    'Num_Op_Collection','Num_Op_CashCredit', 'Num_Op_WithdrawalCash','Num_Op_WithdrawalCreditCard',\n",
    "                    'Num_Sym_Null', 'Num_Sym_Null2','Num_Sym_Pension', 'Num_Sym_Insurance', 'Num_Sym_NegBal',\n",
    "                    'Num_Sym_Household', 'Num_Sym_Statement', 'Num_Sym_IntDep', 'Num_Sym_LoanPayment', \n",
    "                    'Balance_in_negative','Ord_Loan_Payment_amount', 'Ord_Leasing_amount','ratio_urban_inhabitants',\n",
    "                    'unemployment_rate_95','unemployment_rate_96', 'crimes_95_ratio', 'crimes_96_ratio', 'entrepreneurs_ratio' ]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un DataFrame con las variables que vamos a considerar categóricas\n",
    "df_cat=df_original[['account_disponent_bin','frequency', 'sex_owner', 'owner_card_type',\n",
    "       'sex_disponent']]\n",
    "#Vemos que tipos tienen las variables que queremos que sean categóricas\n",
    "df_cat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ponemos las variables \"owner_card_type\" y \"account_disponent_bin\" como string para poder obtener dummies\n",
    "df_cat[\"owner_card_type\"]=df_cat[\"owner_card_type\"].astype(str)\n",
    "df_cat[\"account_disponent_bin\"]=df_cat[\"account_disponent_bin\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dumm=pd.get_dummies(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_dumm.dtypes\n",
    "#Al pasar a dummies las variables, hemos incrementado en 8 el número total de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_num, df_cat_dumm], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamos a generar un primer modelo benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Antes de probar con modelos vamos a ver si reduciendo dimensionalidad conseguimos una primera intuición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = df_original[\"account_loan_bin\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2).fit_transform(X)\n",
    "plt.figure(dpi=120)\n",
    "plt.scatter(pca[y.values==0,0], pca[y.values==0,1], alpha=0.5, label='NO', s=2, color='navy')\n",
    "plt.scatter(pca[y.values==1,0], pca[y.values==1,1], alpha=0.5, label='YES', s=2, color='darkorange')\n",
    "plt.legend()\n",
    "plt.title('Producto préstamo')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Del gráfico anterior no consigo sacar nada en claro\n",
    "pca2 = PCA(n_components=2)\n",
    "pca2.fit(X)\n",
    "print(pca2.components_)\n",
    "print(pca2.explained_variance_ratio_)\n",
    "#De los componentes de momento tampoco sacamos ninguna conclusión. Las variables están en escalas muy distintas y por eso \n",
    "#el PCA genera resultados tan \"positivos\" en cuanto a explicabilidad de las 2 primeras componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasamos a hacer modelos sencillos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos conjuntos de train y de test. Para el test usamos el 20% de las observaciones\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos una Regresión logística\n",
    "clf_LR=LogisticRegression(random_state=0)\n",
    "clf_LR.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_modelo (clf,X_train,y_train, X_test,y_test):\n",
    "    print(\"Datos de train:\")\n",
    "    print(\"El accuracy es\",accuracy_score(y_train,clf.predict(X_train))*100,\"%\")\n",
    "    print(\"La precision es\",precision_score(y_train,clf.predict(X_train))*100, \"%\")\n",
    "    print(\"El recall es\",recall_score(y_train,clf.predict(X_train))*100,\"%\")\n",
    "    tn, fp, fn, tp=confusion_matrix(y_train,clf.predict(X_train)).ravel()\n",
    "    print(\"tn:\",tn,\" fp:\",fp,\" fn:\",fn,\" tp:\",tp)\n",
    "    print(\"Datos de test:\")\n",
    "    print(\"El accuracy es\",accuracy_score(y_test,clf.predict(X_test))*100,\"%\")\n",
    "    print(\"La precision es\",precision_score(y_test,clf.predict(X_test))*100, \"%\")\n",
    "    print(\"El recall es\",recall_score(y_test,clf.predict(X_test))*100,\"%\")\n",
    "    tn_t, fp_t, fn_t, tp_t=confusion_matrix(y_test,clf.predict(X_test)).ravel()\n",
    "    print(\"tn:\",tn_t,\" fp:\",fp_t,\" fn:\",fn_t,\" tp:\",tp_t)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_modelo(clf_LR,X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Probamos con un árbol de decisión\n",
    "clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_modelo(clf_tree,X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Los resultados de estos modelos tan sencillos están siendo extraordinarios. Vamos a investigar las razones.\n",
    "#Además como las clases están desbalanceadas vamos a ir comparando el efecto de incluir oversampling o no. \n",
    "#Vamos a profundizar con Decission Trees en este primer momento ya que tiene menores requerimientos teóricos para las features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Las clases a predecir (si la cuenta tiene prestamo=1 ó no tiene =0) están desbalanceadas\n",
    "y2=pd.DataFrame(y)\n",
    "sns.countplot(x=\"account_loan_bin\",data=y2, palette='hls')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2['account_loan_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y2['account_loan_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos el porcentaje que representa cada clase:\n",
    "print(\"Las cuentas CON préstamo son el\", \"%.2f\" % (y2['account_loan_bin'].value_counts()[0]/len(y2['account_loan_bin'])*100) ,\"%\")\n",
    "print(\"Las cuentas SIN préstamo son el\", \"%.2f\" % (y2['account_loan_bin'].value_counts()[1]/len(y2['account_loan_bin'])*100) ,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Al estar las clases desbalanceadas hay que ir con cuidado porque un modelo que prediga siempre NO tendría un accuracy\n",
    "#de casi el 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a querer dibujar Decision Trees\n",
    "from io import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para dibujar un árbol\n",
    "def dibu_arb(tree):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(tree, out_file=dot_data,filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    return(Image(graph.create_png()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[36]\n",
    "#Parece que la variable \"Num_Sym_LoanPayment\" contiene toda la información \"account_loan_bin\", \n",
    "#aunque se han extraido de ficheros distintos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[clf_tree.feature_importances_>0.10] #Vamos a ver las variables más importantes para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Link interesante para ver como se realiza el cálculo de \"feature_importances\"_\n",
    "#https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c glemaitre imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para ver las columnas que vamos a denominar como categóricas cuando apliquemos SMOTE\n",
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smo=SMOTENC(categorical_features=range(46,59),random_state=0)#Las variables categóricas van a ser de la 46 a la 59\n",
    "os_X,os_y=smo.fit_sample(X_train, y_train)\n",
    "columns = X_train.columns\n",
    "os_X = pd.DataFrame(data=os_X,columns=columns)\n",
    "os_y= pd.DataFrame(data=os_y,columns=['account_loan_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos que SMOTENC funciona como esperábamos\n",
    "\n",
    "print(\"length of oversampled data is \",len(os_X))\n",
    "print(\"Number of loans=0 in oversampled data\",len(os_y[os_y['account_loan_bin']==0]))\n",
    "print(\"Number of loans=1\",len(os_y[os_y['account_loan_bin']==1]))\n",
    "print(\"Proportion of loans=0 is \",len(os_y[os_y['account_loan_bin']==0])/len(os_X))\n",
    "print(\"Proportion of loans=1 is \",len(os_y[os_y['account_loan_bin']==1])/len(os_X))\n",
    "\n",
    "os_bin=os_X[['account_disponent_bin_0','account_disponent_bin_1',\n",
    "       'frequency_After_trans', 'frequency_Monthly', 'frequency_Weekly',\n",
    "       'sex_owner_F', 'sex_owner_M', 'owner_card_type_0', 'owner_card_type_1',\n",
    "       'owner_card_type_2', 'owner_card_type_3', 'sex_disponent_F',\n",
    "       'sex_disponent_M']]\n",
    "print(\"unique de variables categóricas\",unique(os_bin))\n",
    "print(\"unique de variable y\",unique(os_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_tree_os=os_clf_tree.fit(os_X,os_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_modelo(clf_tree_os,os_X,os_y, X_test, y_test)\n",
    "#Obtenemos los mismos resultados con y sin SMOTE. Lógicamente porque estamos suponiendo que la variable \n",
    "#'Num_Sym_LoanPayment' contiene la información de si la cuenta tiene un préstamo o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Vemos que tener un préstamo correlaciona de forma muy significativa con las variables:Ord_Loan_Payment,Num_Sym_LoanPayment y \n",
    "#Ord_Loan_Payment_amount\n",
    "df_original.corr()[\"account_loan_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos las correlaciones de las variables anteriores\n",
    "print(np.corrcoef(df_original[\"account_loan_bin\"],df_original[\"Ord_Loan_Payment\"]))\n",
    "print(np.corrcoef(df_original[\"account_loan_bin\"],df_original[\"Ord_Loan_Payment_amount\"]))\n",
    "print(np.corrcoef(df_original[\"account_loan_bin\"],df_original[\"Num_Sym_LoanPayment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a ver qué resultados obtenemos eliminando la variable 'Num_Sym_LoanPayment'\n",
    "X1=X.drop(['Num_Sym_LoanPayment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_treeX1=X1_clf_tree.fit(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos oversampling\n",
    "smo=SMOTENC(categorical_features=range(45,58),random_state=0)# Modificamos rango porque hemos eliminado una variable\n",
    "os_X1,os_y1=smo.fit_sample(X1_train, y1_train)\n",
    "columns = X1_train.columns\n",
    "os_X1 = pd.DataFrame(data=os_X1,columns=columns)\n",
    "os_y1= pd.DataFrame(data=os_y1,columns=['account_loan_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos que SMOTENC funciona como esperábamos\n",
    "\n",
    "print(\"length of oversampled data is \",len(os_X1))\n",
    "print(\"Number of loans=0 in oversampled data\",len(os_y1[os_y1['account_loan_bin']==0]))\n",
    "print(\"Number of loans=1\",len(os_y1[os_y1['account_loan_bin']==1]))\n",
    "print(\"Proportion of loans=0 is \",len(os_y1[os_y1['account_loan_bin']==0])/len(os_X1))\n",
    "print(\"Proportion of loans=1 is \",len(os_y1[os_y1['account_loan_bin']==1])/len(os_X1))\n",
    "\n",
    "os_bin=os_X1[['account_disponent_bin_0','account_disponent_bin_1',\n",
    "       'frequency_After_trans', 'frequency_Monthly', 'frequency_Weekly',\n",
    "       'sex_owner_F', 'sex_owner_M', 'owner_card_type_0', 'owner_card_type_1',\n",
    "       'owner_card_type_2', 'owner_card_type_3', 'sex_disponent_F',\n",
    "       'sex_disponent_M']]\n",
    "print(\"unique de variables categóricas\",unique(os_bin))\n",
    "print(\"unique de variable y\",unique(os_y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos que el oversampling funciona bien, no lo vamos a chequear las próximas ocasiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osX1_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_tree_osX1=osX1_clf_tree.fit(os_X1,os_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluamos los modelos\n",
    "print(\"Sin Oversampling\")\n",
    "eval_modelo(clf_treeX1,X1_train,y1_train, X1_test, y1_test)\n",
    "print(\"Con Oversampling\")\n",
    "eval_modelo(clf_tree_osX1,os_X1,os_y1, X1_test, y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_treeX1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_tree_osX1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train.columns[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.columns[clf_tree_osX1.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.columns[clf_treeX1.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El modelo continua dando unos resultados espectaculares, vamos a ver que sucede si eliminamos la variable \n",
    "#'Ord_Loan_Payment_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=X1.drop(['Ord_Loan_Payment_amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_treeX2=X2_clf_tree.fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos oversampling\n",
    "smo=SMOTENC(categorical_features=range(44,57),random_state=0)# Modificamos rango porque hemos eliminado una variable\n",
    "os_X2,os_y2=smo.fit_sample(X2_train, y2_train)\n",
    "columns = X2_train.columns\n",
    "os_X2 = pd.DataFrame(data=os_X2,columns=columns)\n",
    "os_y2= pd.DataFrame(data=os_y2,columns=['account_loan_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osX2_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_tree_osX2=osX2_clf_tree.fit(os_X2,os_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluamos los modelos\n",
    "print(\"Sin Oversampling\")\n",
    "eval_modelo(clf_treeX2,X2_train,y2_train, X2_test, y2_test)\n",
    "print(\"Con Oversampling\")\n",
    "eval_modelo(clf_tree_osX2,os_X2,os_y2, X2_test, y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_treeX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_tree_osX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train.columns[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.columns[clf_treeX2.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.columns[clf_tree_osX2.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El modelo continua dando resultados espectaculares, vamos a ver que sucede si eliminamos la variable Ord_Loan_Payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3=X2.drop(['Ord_Loan_Payment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_treeX3=X3_clf_tree.fit(X3_train,y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos oversampling\n",
    "smo=SMOTENC(categorical_features=range(43,56),random_state=0)# Modificamos rango porque hemos eliminado una variable\n",
    "os_X3,os_y3=smo.fit_sample(X3_train, y3_train)\n",
    "columns = X3_train.columns\n",
    "os_X3 = pd.DataFrame(data=os_X3,columns=columns)\n",
    "os_y3= pd.DataFrame(data=os_y3,columns=['account_loan_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osX3_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_tree_osX3=osX3_clf_tree.fit(os_X3,os_y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluamos los modelos\n",
    "print(\"Sin Oversampling\")\n",
    "eval_modelo(clf_treeX3,X3_train,y3_train, X3_test, y3_test)\n",
    "print(\"Con Oversampling\")\n",
    "eval_modelo(clf_tree_osX3,os_X3,os_y3, X3_test, y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con Oversampling estoy \"forzando\" al modelo a generar más positivos  (tp y fp), y en este caso se ve perfectamente como\n",
    "# con oversampling mejoro el recall (porque tengo más positivos), pero empeoro la precision porque no los estoy prediciendo\n",
    "#correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_treeX3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dibu_arb(clf_tree_osX3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X3_train.columns[19])#Número de un tipo especial de reintegros\n",
    "print(X3_train.columns[22])#Número de transferencias enviadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3.columns[clf_treeX3.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3.columns[clf_tree_osX3.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_treeX3.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf_treeX3.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_original.corr()[\"account_loan_bin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Después de eliminar las variables que más suenan a \"préstamo\", vamos a ver que sucede si eliminamos la variable Num_Type_VYBER,\n",
    "#que es la de mayor importancia.Esta variable se corresponde con un tipo especial de reintegros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4=X3.drop(['Num_Type_VYBER'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_treeX4=X4_clf_tree.fit(X4_train,y4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicamos oversampling\n",
    "smo=SMOTENC(categorical_features=range(42,55),random_state=0)# Modificamos rango porque hemos eliminado una variable\n",
    "os_X4,os_y4=smo.fit_sample(X4_train, y4_train)\n",
    "columns = X4_train.columns\n",
    "os_X4 = pd.DataFrame(data=os_X4,columns=columns)\n",
    "os_y4= pd.DataFrame(data=os_y4,columns=['account_loan_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osX4_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)\n",
    "clf_tree_osX4=clf_tree.fit(os_X4,os_y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluamos los modelos\n",
    "print(\"Sin Oversampling\")\n",
    "eval_modelo(clf_treeX4,X4_train,y4_train, X4_test, y4_test)\n",
    "print(\"Con Oversampling\")\n",
    "eval_modelo(clf_tree_osX4,os_X4,os_y4, X4_test, y4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con los datos de X4 vemos que el oversampling genera el mismo efecto que en el caso de X3, mejora recall y empeora precision.\n",
    "#Además, al eliminar la variable de 'Num_Type_VYBER' es destacable que el modelo que obtenemos tiende a generar un mayor número \n",
    "#de positivos, pero estos positivos resultan ser falsos positivos en su mayoría "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Puedo considerar que el oversampling no me está aportando nada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_treeX4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_tree_osX4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me quedo con el DataSet X3 no elimino la variable Num_Type_VYBER, ya que los resultados, aunque son peores, son ahora\n",
    "# más razonables (teniendo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a seguir con árboles ya que es un modelo sencillo y que podría aportar explicabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a hacer scaling, aplicado a árboles no debería tener un gran impacto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a probar Robust Scaling, partiendo del DataFrame X3 (después de eliminar las variables que parece que contenían \n",
    "#la información de si una cuenta ha contratado préstamo o no lo ha contratado)\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "rbs = RobustScaler()\n",
    "columns = X3.columns\n",
    "rbs_scale = rbs.fit_transform(X3)\n",
    "X=pd.DataFrame(rbs_scale,columns=columns)\n",
    "X_rbs_sca=X.copy #Por si luego lo utilizamos\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)#Con scaling\n",
    "clf_tree=X_clf_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparamos los modelos con y sin scaling\n",
    "print(\"Sin Scaling\")\n",
    "eval_modelo(clf_treeX3,X3_train,y3_train, X3_test, y3_test)\n",
    "print(\"Con Scaling\")\n",
    "eval_modelo(X_clf_tree,X_train,y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los resultados con Robust Scaling no mejoran, así que vamos a probar con Standard Scaling aunque en un modelo de\n",
    "# Decision Tree este modificación acostumbrará a tener poco impacto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scl = StandardScaler()\n",
    "columns = X3.columns\n",
    "scl_scale = scl.fit_transform(X3)\n",
    "X=pd.DataFrame(scl_scale,columns=columns)\n",
    "X_sta_sca=X.copy #Por si luego lo utilizamos\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)#Con scaling\n",
    "clf_tree_=X_clf_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparamos los modelos con y sin scaling\n",
    "print(\"Sin Scaling\")\n",
    "eval_modelo(clf_treeX3,X3_train,y3_train, X3_test, y3_test)\n",
    "print(\"Con Scaling\")\n",
    "eval_modelo(clf_tree,X_train,y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a probar si con el scaling, cuando hacemos PCA podemos obtener una representación simple de los datos. \n",
    "#Sé que va a ser difícil pero lo probamos.\n",
    "pca = PCA(n_components=2).fit_transform(X)\n",
    "plt.figure(dpi=120)\n",
    "plt.scatter(pca[y.values==0,0], pca[y.values==0,1], alpha=0.5, label='NO', s=2, color='navy')\n",
    "plt.scatter(pca[y.values==1,0], pca[y.values==1,1], alpha=0.5, label='YES', s=2, color='darkorange')\n",
    "plt.legend()\n",
    "plt.title('Producto préstamo')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Del gráfico anterior no consigo sacar nada en claro\n",
    "pca2 = PCA(n_components=2)\n",
    "pca2.fit(X)\n",
    "print(pca2.components_)\n",
    "print(pca2.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De las componentes principales de momento tampoco sacamos ninguna conclusión, pero al haber escalado los datos ahora obtenemos un dato \n",
    "#más razonable de variabilidad explicada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaling tampoco ha funcionado mejor que sin scaling. Aplicamos scaling sólo a las variables numéricas\n",
    "print(X3.columns)\n",
    "print(X3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos las variables numéricas de las que consideramos categóricas\n",
    "X3_cat=X3[['account_disponent_bin_0','account_disponent_bin_1',\n",
    "       'frequency_After_trans', 'frequency_Monthly', 'frequency_Weekly',\n",
    "       'sex_owner_F', 'sex_owner_M', 'owner_card_type_0', 'owner_card_type_1',\n",
    "       'owner_card_type_2', 'owner_card_type_3', 'sex_disponent_F',\n",
    "       'sex_disponent_M']]\n",
    "X3_num=X3[['Date_Account', 'birth_owner', 'birth_disponent', 'owner_card_date',\n",
    "       'Ord_Insurance', 'Ord_Insurance_amount', 'Ord_Household_Payment',\n",
    "       'Ord_Household_Payment_amount', 'Ord_Leasing', 'Ord_Empty',\n",
    "       'Ord_Empty_amount', 'num_inhabitants', 'municip < 499',\n",
    "       'municip 500-1999', 'municip 2000-9999', 'municip > 10000',\n",
    "       'num_cities', 'avg_salary', 'Num_Type_Credit', 'Num_Type_VYBER',\n",
    "       'Num_Type_Withdrawal', 'Num_Op_Null', 'Num_Op_Remittances',\n",
    "       'Num_Op_Collection', 'Num_Op_CashCredit', 'Num_Op_WithdrawalCash',\n",
    "       'Num_Op_WithdrawalCreditCard', 'Num_Sym_Null', 'Num_Sym_Null2',\n",
    "       'Num_Sym_Pension', 'Num_Sym_Insurance', 'Num_Sym_NegBal',\n",
    "       'Num_Sym_Household', 'Num_Sym_Statement', 'Num_Sym_IntDep',\n",
    "       'Balance_in_negative', 'Ord_Leasing_amount', 'ratio_urban_inhabitants',\n",
    "       'unemployment_rate_95', 'unemployment_rate_96', 'crimes_95_ratio',\n",
    "       'crimes_96_ratio', 'entrepreneurs_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos con el Robust Scaling\n",
    "columns = X3_num.columns\n",
    "X3_scale = rbs.fit_transform(X3_num)\n",
    "X3_scale=pd.DataFrame(X3_scale,columns=columns)\n",
    "X = pd.concat([X3_scale,X3_cat], axis = 1)\n",
    "X.shape\n",
    "X_sca_num=X.copy #Guardamos este DataFrame por si la utilizamos posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)#Con scaling\n",
    "clf_tree_=X_clf_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparamos los modelos con y sin scaling\n",
    "print(\"Sin Scaling\")\n",
    "eval_modelo(clf_treeX3,X3_train,y3_train, X3_test, y3_test)\n",
    "print(\"Con Robust Scaling sólo en variables numéricas\")\n",
    "eval_modelo(clf_tree,X_train,y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En este caso, el robust scaling tampoco mejora los resultados que teníamos originalmente. Probamos otra alternativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probamos con min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "minmax = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X3_num.columns\n",
    "X3_scale = minmax.fit_transform(X3_num)\n",
    "X3_scale=pd.DataFrame(X3_scale,columns=columns)\n",
    "X = pd.concat([X3_scale,X3_cat], axis = 1)\n",
    "X.shape\n",
    "Xminmax=X.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=5,random_state=0)#Con scaling\n",
    "clf_tree_=X_clf_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Comparamos los modelos con y sin scaling\n",
    "print(\"Sin Scaling\")\n",
    "eval_modelo(clf_treeX3,X3_train,y3_train, X3_test, y3_test)\n",
    "print(\"Con Scaling\")\n",
    "eval_modelo(clf_tree,X_train,y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con scaling MinMax el modelo casi no genera predicciones positivas y el accuracy se obtiene por un modelo que básicamente\n",
    "#predice en test que la cuenta no contrata préstamo y al estar las clases desbalanceadas obtenemos ese acuraccy, pero muy mal\n",
    "#recall y precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasta el momento hemos visto que en el dataset que habíamos generado \n",
    "habían 3 variables que \"contenían\" la información de la variable que queríamos predecir y también hemos visto que, en el caso de Decision Trees, las técnicas de Oversampling y de Scaling tal y como las hemos aplicado no mejoran los modelos iniciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A continuación trabajamos sobre las features numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X3_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A partir del DataFrame X3 creamos un DataFrame que contenga el log natural de las variables numéricas\n",
    "cols = X3_num.columns\n",
    "X3_log=pd.DataFrame(X3_num,columns=columns)#También podríamos hacer un copy del DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos el logaritmo de las variables que consideramos numéricas\n",
    "cols=X3_num.columns\n",
    "for col in cols:\n",
    "    X3_log[col]=X3_log[col]+1.1 #Para evitar negativos al aplicar el logaritmo añadimos 1.1\n",
    "    X3_log[col]=np.log(X3_log[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hemos conseguido diminuir la variabilidad de las features\n",
    "X3_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un DataFrame a partir del DataFrame X3 (una vez eliminadas las 3 variables que contenían la información de la variable\n",
    "#a predecir), aplicando logaritmo a las variables que consideramos numéricas y dejando las categóricas igual.\n",
    "X_log= pd.concat([X3_log,X3_cat], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a crear un nuevo DataFrame binarizando las variables que tienen el número de operaciones (pasan de tener el número de \n",
    "#operaciones a decir si la cuenta ha realizado ese tipo de operativa o no) y eliminando las columnas que contienen\n",
    "#el sufijo \"_amount\" (sólo queremos reflejar si se ha realizado un tipo de operativa o no se ha realizado)\n",
    "\n",
    "col_to_bin=['Ord_Insurance', 'Ord_Household_Payment','Ord_Leasing', 'Ord_Empty','Num_Type_Credit', 'Num_Type_VYBER',\n",
    "       'Num_Type_Withdrawal', 'Num_Op_Null', 'Num_Op_Remittances','Num_Op_Collection', 'Num_Op_CashCredit', \n",
    "       'Num_Op_WithdrawalCash','Num_Op_WithdrawalCreditCard', 'Num_Sym_Null', 'Num_Sym_Null2',\n",
    "       'Num_Sym_Pension', 'Num_Sym_Insurance', 'Num_Sym_NegBal','Num_Sym_Household', 'Num_Sym_Statement', 'Num_Sym_IntDep',\n",
    "       'Balance_in_negative']\n",
    "\n",
    "col_out=['Ord_Insurance_amount','Ord_Household_Payment_amount','Ord_Empty_amount', 'Ord_Leasing_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X3.columns\n",
    "X3_bin=pd.DataFrame(X3,columns=cols)\n",
    "X3_bin=X3_bin.drop(col_out, axis=1)#Eliminamos las columnas de amount\n",
    "X3_bin=X3_bin.drop(col_to_bin, axis=1)#Eliminamos las columnas a binarizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el dataset que queremos binarizar\n",
    "X3_to_bin=X3[['Ord_Insurance', 'Ord_Household_Payment','Ord_Leasing', 'Ord_Empty','Num_Type_Credit', 'Num_Type_VYBER',\n",
    "       'Num_Type_Withdrawal', 'Num_Op_Null', 'Num_Op_Remittances','Num_Op_Collection', 'Num_Op_CashCredit', \n",
    "       'Num_Op_WithdrawalCash','Num_Op_WithdrawalCreditCard', 'Num_Sym_Null', 'Num_Sym_Null2',\n",
    "       'Num_Sym_Pension', 'Num_Sym_Insurance', 'Num_Sym_NegBal','Num_Sym_Household', 'Num_Sym_Statement', 'Num_Sym_IntDep',\n",
    "       'Balance_in_negative']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binario(x):\n",
    "    if x>0:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_to_bin:\n",
    "    X3_to_bin[col]=X3_to_bin[col].apply(binario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X3_to_bin=X3_to_bin.astype(str)\n",
    "X3_to_bin=pd.get_dummies(X3_to_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Porque hay algunas columnas que no se duplican con el get_dummies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un DataFrame a partir del DataFrame X3 (una vez eliminadas las 3 variables que conteníanla información de la variable\n",
    "#dependiente), binarizando las variables que contaban el número de operaciones.\n",
    "X_bin= pd.concat([X3_bin,X3_to_bin], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Con los nuevos DataFrames de features vamos a intentar encontrar el mejor Decision Tree y ver cuales son las variables más significativas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Los DataFrames con los que vamos a trabajar son:\n",
    "#X_orig: El Dataframe con el que habíamos trabajado antes eliminando 3 variables, también lo llamábamos X3.\n",
    "X=X3\n",
    "#X_log: Es el Dataframe X_orig pero haciendo el logaritmo natural a las variables numéricas\n",
    "#X_bin: Es el Dataframe X_orig pero binarizando las variables que contaban cuantas operaciones de cada tipo se habían realizado\n",
    "#en cada cuenta, ahora decimos si la cuenta tiene ese tipo de operativa o no, y eliminando las variables que cuantificaban los\n",
    "#importes de dicha operativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicialmente pensaba considerar como única métrica la precisión (quería que el modelo minimizase los falsos positivos,\n",
    "#pero en vista de los resultados de los modelos anteriores, en los que el recall (el % de positivos que acierto) \n",
    "#puede ser muy bajo voy a comenzar a monitorizar también el F1 Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Incluimos la métrica del F1 Score en la evaluación de los modelos\n",
    "def eval_modelo_2 (clf,X_train,y_train, X_test,y_test):\n",
    "    print(\"Datos de train:\")\n",
    "    print(\"El accuracy es\",accuracy_score(y_train,clf.predict(X_train))*100,\"%\")\n",
    "    print(\"La precision es\",precision_score(y_train,clf.predict(X_train))*100, \"%\")\n",
    "    print(\"El recall es\",recall_score(y_train,clf.predict(X_train))*100, \"%\")\n",
    "    print(\"El F1 Score es\",f1_score(y_train,clf.predict(X_train))*100,\"%\")\n",
    "    tn, fp, fn, tp=confusion_matrix(y_train,clf.predict(X_train)).ravel()\n",
    "    print(\"tn:\",tn,\" fp:\",fp,\" fn:\",fn,\" tp:\",tp)\n",
    "    print(\"Datos de test:\")\n",
    "    print(\"El accuracy es\",accuracy_score(y_test,clf.predict(X_test))*100,\"%\")\n",
    "    print(\"La precision es\",precision_score(y_test,clf.predict(X_test))*100, \"%\")\n",
    "    print(\"El recall es\",recall_score(y_test,clf.predict(X_test))*100,\"%\")\n",
    "    print(\"El F1 Score es\",f1_score(y_test,clf.predict(X_test))*100,\"%\")\n",
    "    tn_t, fp_t, fn_t, tp_t=confusion_matrix(y_test,clf.predict(X_test)).ravel()\n",
    "    print(\"tn:\",tn_t,\" fp:\",fp_t,\" fn:\",fn_t,\" tp:\",tp_t)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a definir la parrilla para realizar Randomized Grid Search\n",
    "\n",
    "# Máximo número de niveles en el árbol. Damos una distribución con mayor probabilidad en valores pequeños\n",
    "max_depth1 = [int(x) for x in np.linspace(2, 20, num = 10)]#\"Sobreponderamos\" árboles con poca profundidad\n",
    "max_depth2 =[int(x) for x in np.linspace(30, 100, num = 4)]\n",
    "max_depth=max_depth1 + max_depth2\n",
    "\n",
    "# Mínimo número de observaciones en cada hoja.Damos una distribución con mayor probabilidad en valores pequeños\n",
    "min_samples_leaf_1 = [int(x) for x in np.linspace(5, 50, num = 4)]\n",
    "min_samples_leaf_2 = [int(x) for x in np.linspace(60, 100, num = 10)]\n",
    "min_samples_leaf=min_samples_leaf_1+min_samples_leaf_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la grid aleatoria\n",
    "random_grid = {'max_depth': max_depth,\n",
    "               'min_samples_leaf': min_samples_leaf\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree = DecisionTreeClassifier(random_state=0)\n",
    "#Creamos una versión para optimizar la precision\n",
    "clf_tree_random_p= RandomizedSearchCV(random_state=0,estimator = clf_tree, param_distributions = random_grid, n_iter = 100, cv = 5,scoring=\"precision\")\n",
    "#Creamos una versión para optimizar el F1 Score\n",
    "clf_tree_random_f1= RandomizedSearchCV(random_state=0,estimator = clf_tree, param_distributions = random_grid, n_iter = 100, cv = 5,scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comenzamos por el DataFrame X (el X3 anterior)\n",
    "#Generamos conjuntos de train y el de test. Para el test usamos el 20% de las observaciones\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_tree_random_p.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_random_p.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trabajamos ahora con el mejor modelo encontrado en el Randomized Search, optimizando la precision. Entrenamos ahora el modelo\n",
    "#con todos los datos de train y luego lo evaluaremos con un conjunto de test no utilizado en la estimación\n",
    "clf_tree_p = clf_tree_random_p.best_estimator_\n",
    "clf_tree_p.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_modelo_2 (clf_tree_p,X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parece que el modelo anterior produce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[clf_tree_p.feature_importances_>0.10]\n",
    "#Las variables que obtenemos de este árbol que parece que produce overfitting son las que habíamos visto anteriormente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos ahora los resultados con la optimización realizada sobre F1 Score\n",
    "\n",
    "clf_tree_random_f1.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_random_f1.best_params_)\n",
    "\n",
    "clf_tree_f1 = clf_tree_random_f1.best_estimator_  \n",
    "clf_tree_f1.fit(X_train,y_train)\n",
    "\n",
    "eval_modelo_2 (clf_tree_f1,X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atención: obtengo mejor precision en el modelo que optimiza F1 Score que en el que optimiza Precision. Dado que el coste  \n",
    "#computacional es bajo, creo que va a ser mejor aplicar GridSearch CV en lugar de RandomizedGridSearchCV, para no obtener \n",
    "#resultados incoherentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparando el modelo que optimizamos el F1 Score, respecto al que optimizamos la precision, creo que es mejor el que optimiza\n",
    "#el F1 Score porque la precision puede bajar un poco, el recall aumenta de forma más significativa. Esto lo consigue \n",
    "#generando un mayor número de positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomamos el árbol anterior como árbol de referencia ya que ofrece el mejor balance de precision-recall\n",
    "clf_tree_best=clf_tree_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[clf_tree_f1.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parece que los modelos anteriores overfittean. Vamos a probar un modelo más sencillo\n",
    "clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=3,random_state=0)\n",
    "clf_tree.fit(X_train,y_train)\n",
    "eval_modelo_2(clf_tree,X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree_benchmark=clf_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_tree_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.columns[19])\n",
    "print(X_train.columns[25])\n",
    "print(X_train.columns[22])\n",
    "print(X_train.columns[24])\n",
    "print(X_train.columns[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpretando el árbol anterior, lo que obtenemos es que, mayoritariamente, las cuentas que contratan préstamos son las que:\n",
    "#--Num_Type_Viber > 0.5 (reintegros del tipo especial Vyber)\n",
    "#--Num_Op_Remittances > 0.5 (envios/transferencias a otros bancos). Este podría ser un indicador de la solvencia y por tanto\n",
    "#le encuentro sentido a su aparición\n",
    "#--Ord_Leasing Amount<=501,5 (cargos automáticos por Leasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dibu_arb(clf_tree_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[clf_tree_f1.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_importance(clf_tree_f1)\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De momento parece que la información aportada por los modelos obtenidos con randomized search y un modelo sencillo con\n",
    "#criterio experto aportan resultados bastante similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comenzamos por el DataFrame X_log\n",
    "#Generamos conjuntos de train y de test. Para el test usamos el 20% de las observaciones\n",
    "X_log_train, X_log_test, y_log_train, y_log_test = train_test_split(X_log, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a seguir trabajando con árboles por lo que, como en el caso del scaling, no esperamos mejoras significativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree_random_p.fit(X_log_train, y_log_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_random_p.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree_p = clf_tree_random_p.best_estimator_  \n",
    "clf_tree_p.fit(X_log_train,y_log_train)\n",
    "eval_modelo_2(clf_tree_p,X_log_train, y_log_train,X_log_test, y_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[clf_tree_p.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizamos ahora el F1 Score\n",
    "clf_tree_random_f1.fit(X_log_train, y_log_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_random_f1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree_f1 = clf_tree_random_f1.best_estimator_  \n",
    "clf_tree_f1.fit(X_log_train, y_log_train)\n",
    "eval_modelo_2 (clf_tree_f1,X_log_train, y_log_train,X_log_test, y_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atención: obtengo mejor precision en el modelo que optimiza F1 Score que en el que optimiza Precision. Dado que el coste  \n",
    "#computacional es bajo, creo que va a ser mejor aplicar GridSearch CV en lugar de RandomizedGridSearchCV, para no obtener \n",
    "#resultados incoherentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log.columns[clf_tree_f1.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parece que los modelos anteriores overfittean. Vamos a probar un modelo más sencillo\n",
    "clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=3,random_state=0)\n",
    "clf_tree.fit(X_log_train, y_log_train)\n",
    "eval_modelo_2(clf_tree,X_log_train, y_log_train,X_log_test, y_log_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log.columns[clf_tree_f1.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos que hemos obtenido features muy similares probando con técnicas más complejas como Randomized Search y con \n",
    "#criterio experto o lo que podríamos haber obtenido a partir de un modelo benchmark inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seguimos con el DataFrame X_bin, en el que hemos binarizado las variables de \n",
    "#Generamos conjuntos de train y de test. Para el test usamos el 20% de las observaciones\n",
    "X_bin_train, X_bin_test, y_bin_train, y_bin_test = train_test_split(X_bin, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree_random_p.fit(X_bin_train, y_bin_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_random_p.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree_p = clf_tree_random_p.best_estimator_  \n",
    "clf_tree_p.fit(X_bin_train,y_bin_train)\n",
    "eval_modelo_2(clf_tree_p,X_bin_train, y_bin_train,X_bin_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin.columns[clf_tree_p.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizamos ahora el F1 Score\n",
    "clf_tree_random_f1.fit(X_bin_train, y_bin_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_random_f1.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_tree_f1 = clf_tree_random_f1.best_estimator_  \n",
    "clf_tree_f1.fit(X_bin_train, y_bin_train)\n",
    "eval_modelo_2 (clf_tree_f1,X_bin_train, y_bin_train,X_bin_test, y_bin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con este DataFrame, los resultados son claramente los peores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bin.columns[clf_tree_f1.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Los DataFrames aplicando logaritmo y binarizando no mejoran el resultado del DataFrame original, se puede entender porque el\n",
    "#modelo que estamos aplicando es un árbol.\n",
    "\n",
    "#También estamos viendo que consistentemente las variables que son más relevantes son:\n",
    "#--Num_Type_VYBER\n",
    "#--Num_Op_Remittances\n",
    "#--Ord_Household_Payment_amount\n",
    "#--Ord_Leasing\n",
    "\n",
    "#A continuación vamos a aplicar GridSearch con el conjunto de datos procedente de X3 (datos originales menos 3 variables)\n",
    "#y vamos a ver que variables nos salen como más relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos la misma matriz de hiperparámetros\n",
    "grid=random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=StratifiedKFold(n_splits=5, random_state=0,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los modelos para hacer la búsqueda de hiperparámetros \n",
    "clf_tree = DecisionTreeClassifier(random_state=0)\n",
    "#Creamos una versión para optimizar la precision\n",
    "clf_tree_gridsearch_p= GridSearchCV(estimator = clf_tree, param_grid = grid, cv=k, scoring=\"precision\")\n",
    "#Creamos una versión para optimizar el F1 Score\n",
    "clf_tree_gridsearch_f1= GridSearchCV(estimator = clf_tree, param_grid = grid, cv=k, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos ahora los resultados con la optimización realizada sobre Precision\n",
    "\n",
    "clf_tree_gridsearch_p.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_gridsearch_p.best_params_)\n",
    "\n",
    "clf_tree_gs_p = clf_tree_gridsearch_p.best_estimator_  \n",
    "clf_tree_gs_p.fit(X_train,y_train)\n",
    "\n",
    "eval_modelo_2 (clf_tree_gs_p,X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos las features de mayor relvancia en el modelo\n",
    "X.columns[clf_tree_gs_p.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos ahora los resultados con la optimización realizada sobre F1 Score\n",
    "\n",
    "clf_tree_gridsearch_f1.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_gridsearch_f1.best_params_)\n",
    "\n",
    "clf_tree_gs_f1 = clf_tree_gridsearch_f1.best_estimator_\n",
    "clf_tree_gs_f1.fit(X_train,y_train)\n",
    "\n",
    "eval_modelo_2 (clf_tree_gs_f1,X_train,y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo puede ser que obtenga un mejor resultado en Precision cuando optimizo F1 que cuando optimizo precision?. Creo que tengo \n",
    "todos los Random State controlados. Entonces puede ser que al volver  entrenar el modelo con el conjunto completo de los datos\n",
    "de train, cambie el modelo y su evaluación.\n",
    "Aunque parece que el modelo produce overfitting, de momento vamos a considerar este modelo para realizar las interpretaciones, ya que en datos de test es el que mejor resultados muestra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos la relevancia de las features en el modelo\n",
    "clf_tree_gs_f1.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos que muchas features tienen relevancia cero  o prácticamente nula \n",
    "#Veamos las features de mayor relevancia en el modelo\n",
    "X.columns[clf_tree_gs_f1.feature_importances_>0.10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizamos a continuación la interpretabilidad de los resultados obtenidos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para ver el impacto de las variables en la probabilidad de que una cuenta tenga un préstamo vamos a aplicar Partial Dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pdpbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdpbox import pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ruta interesante para entender el Partial Dependece Plot\n",
    "#https://christophm.github.io/interpretable-ml-book/pdp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#También vamos a utilizar Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap==0.23.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -I shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comenzamos el análisis de los SHAP values\n",
    "explainer = shap.TreeExplainer(clf_tree_gs_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si intentamos realizar el análisis de shap para el conjunto de Train, nos dice que son demasiadas observaciones a considerar.\n",
    "#Nos recomienda que no tomemos más de 3.000. Tomamos 2.900 de los 3.600 que tenemos en total.\n",
    "X_train_random=X_train.sample(n=2900, random_state=0)\n",
    "shap_values_train = explainer.shap_values(X_train_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparamos tambien los shap values para el test\n",
    "shap_values_test = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de variable Num_Type_VYBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos el impacto de 'Num_Type_VYBER' en X_train con Partial Dependece Plot\n",
    "pdp_goals = pdp.pdp_isolate(model=clf_tree_gs_f1, dataset=X_train, model_features=X.columns, feature='Num_Type_VYBER')\n",
    "pdp.pdp_plot(pdp_goals, 'Num_Type_VYBER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos el impacto de 'Num_Type_VYBER' en X_test con Partial Dependece Plot\n",
    "pdp_goals = pdp.pdp_isolate(model=clf_tree_gs_f1, dataset=X_test, model_features=X.columns, feature='Num_Type_VYBER')\n",
    "pdp.pdp_plot(pdp_goals, 'Num_Type_VYBER')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de Num_Type_VYBER, vemos que el tener este tipo de operativa incrementa la posibilidad de haber contratado\n",
    "un préstamo hasta en un 20%, aproximadamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos la distribución de la variable y en cuantas observaciones tiene efecto esta feature\n",
    "plt.hist(X['Num_Type_VYBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X['Num_Type_VYBER']>0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Para asegurar la consistencia del análisis de Partial Dependence vamos a analizar la correlación de esta variable con el resto\n",
    "#de variables\n",
    "X.corr()[\"Num_Type_VYBER\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que \"Num_Type_VYBER\" tiene correlación que podría ser elevada con \"Num_Sym_Null\" y \"Num_Op_WithdrawalCash\", entorno\n",
    "a un 0,50.\n",
    "\n",
    "Vamos a ver que conclusiones podemos obtener con SHAP Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP Values para  conjunto de train. En los desplegables de X e Y hay que seleccionar la variable Num_Type_VYBER\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_train[1], X_train_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SHAP Values para  conjunto de test. En los desplegables de X e Y hay que seleccionar la variable Num_Type_VYBER\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_test[1], X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en el caso de SHAP values, la relación entre el número de operaciones de tipo Vyber y su contribución a la probabilidad de contratar un préstamo no es monotónico. En general, para valores bajos de Num_Type_VYBER, esta característica\n",
    "incrementa la posibilidad de haber contratado préstamo, pero para valores elevados de esta variable hay parece contribuir negativamente. \n",
    "\n",
    "El scatter plot que se muestra a continuación también muestra que las cuentas con mayor operativa de tipo Vyber, tienden a no haber contratado préstamos. Esta casuísitica debería analizarse en mayor profundidad, revisando dichas cuentas y viendo por ejemplo si contratan otro tipo de productos de crédito que no estén entrando en el fichero de loans.\n",
    "\n",
    "Vemos si podrían ser los leasings, pero parece que las cuentas con mayor operativa de tipo Vyber, tienden a no tener domiciliaciones por leasings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.Num_Type_VYBER, y)\n",
    "plt.xlabel(\"Num_Type_VYBER\")\n",
    "plt.ylabel(\"account_loan_bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.Num_Type_VYBER, X.Ord_Leasing)\n",
    "plt.xlabel(\"Num_Type_VYBER\")\n",
    "plt.ylabel(\"Ord_Leasing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de variable Num_Op_Remittances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Vemos el impacto de 'Num_Op_Remittances' en X_train con Partial Dependece Plot\n",
    "pdp_goals = pdp.pdp_isolate(model=clf_tree_gs_f1, dataset=X_train, model_features=X.columns, feature='Num_Op_Remittances')\n",
    "pdp.pdp_plot(pdp_goals, 'Num_Op_Remittances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Vemos el impacto de 'Num_Op_Remittances' en X_test con Partial Dependece Plot\n",
    "pdp_goals = pdp.pdp_isolate(model=clf_tree_gs_f1, dataset=X_test, model_features=X.columns, feature='Num_Op_Remittances')\n",
    "pdp.pdp_plot(pdp_goals, 'Num_Op_Remittances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de Num_Op_Remittances, vemos que el tener este tipo de operativa puede incrementar la posibilidad \n",
    "de haber contratado un préstamo hasta en más del 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos la distribución de la variable y en cuantas observaciones tiene efecto esta feature\n",
    "plt.hist(X['Num_Op_Remittances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X['Num_Op_Remittances']>0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Para asegurar la consistencia del análisis de Partial Dependence vamos a analizar la correlación de esta variable con el resto\n",
    "#de variables\n",
    "X.corr()[\"Num_Op_Remittances\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que \"Num_Op_Remittances\" tiene correlación que podría ser elevada con \"Num_Sym_Null2\" (0,85), y \"Num_Type_Withdrawal\" (0,83) y condependence otras como \"Num_Sym_Household\" y \"Ord_Insurance\". Estas elevadas correlaciones pueden invalidar los resultados del análisis de Partial Dependence.\n",
    "\n",
    "Vamos a ver que conclusiones podemos obtener con SHAP Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP Values para  conjunto de train. En los desplegables de X e Y hay que seleccionar la variable Num_Op_Remittances\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_train[1], X_train_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SHAP Values para  conjunto de test. En los desplegables de X e Y hay que seleccionar la variable Num_Op_Remittances\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_test[1], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.Num_Op_Remittances, y)\n",
    "plt.xlabel(\"Num_Op_Remittances\")\n",
    "plt.ylabel(\"account_loan_bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un número bajo de opdraciones de Remittances/Envíos de dinero no parece implicar mayor probabilidad de contratar un préstamo (es normal ya que la mayoría de cuentas tienen un número bajo de Remittances y no tienen préstamo). En cambio un número elevado de Remittances sí que implica una mayor probabilidad de haber contratado un préstamo.\n",
    "\n",
    "En mi opinión lo anterior tiene 2 lecturas, realizar un gran número de envíos, puede ser una muestra de calidad crediticia y por tanto de ser elegible para obtener un crédito. Pero también podría guardar relación con que la cuenta haya cotnratado un préstamo y sea la forma de pago (a pesar de haber eliminado inicialmente las variables: Ord_Loan_Payment,Num_Sym_LoanPayment y \n",
    "Ord_Loan_Payment_amount). En consecuencia se debería profundizar en la finalidad de las operaciones de Remittance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de variable Ord_Leasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos el impacto de 'Ord_Leasing' en X_train con Partial Dependece Plot\n",
    "pdp_goals = pdp.pdp_isolate(model=clf_tree_gs_f1, dataset=X_train, model_features=X.columns, feature='Ord_Leasing')\n",
    "pdp.pdp_plot(pdp_goals, 'Ord_Leasing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pdp_goals = pdp.pdp_isolate(model=clf_tree_gs_f1, dataset=X_test, model_features=X.columns, feature='Ord_Leasing')\n",
    "pdp.pdp_plot(pdp_goals, 'Ord_Leasing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de Ord_Leasing, vemos que el tener este tipo de operativa disminuye la probabilidad de haber contratado un préstamo en un 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos la distribución de la variable y en cuantas observaciones tiene efecto esta feature\n",
    "plt.hist(X['Ord_Leasing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X['Ord_Leasing']>0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para asegurar la consistencia del análisis de Partial Dependence vamos a analizar la correlación de esta variable con el resto\n",
    "#de variables\n",
    "X.corr()[\"Ord_Leasing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que \"Ord_Leasing\" tiene correlación elevada con \"Ord_Leasing_amount \" (0,88) ya ambas son distintas de cero en los mismos casos y a medida que una de ellas crece la otra también debería hacerlo. Y la siguiente mayor correlación es con \"Num_Type_VYBER\" de 0,24. Si vemos es scatter plot de \"Ord_Leasing\" y \"Num_Type_VYBER\", vemos que valores altos de \"Num_Type_VYBER\" que \"perjudican\" la probabilidad de haber contratado un préstamo acostumbran a tener \"Ord_Leasing\"=0, que es un valor que no \"perjudica\" la probabilidad de haber contratado un préstamo y por tanto no podemos asumir que ambas variables contengan la misma información\n",
    "\n",
    "A continuación, vamos a ver que conclusiones podemos obtener con SHAP Values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.Num_Type_VYBER, X.Ord_Leasing)\n",
    "plt.xlabel(\"Num_Type_VYBER\")\n",
    "plt.ylabel(\"Ord_Leasing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP Values para  conjunto de train. En los desplegables de X e Y hay que seleccionar la variable Ord_Leasing\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_train[1], X_train_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHAP Values para  conjunto de test. En los desplegables de X e Y hay que seleccionar la variable Ord_Leasing\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_test[1], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.Ord_Leasing, y)\n",
    "plt.xlabel(\"Ord_Leasing\")\n",
    "plt.ylabel(\"account_loan_bin\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tener Ord_Leasing=0, mejora ligeramente la probabilidad de haber contratado un préstamo, mientras que Ord_Leasing=1 disminuye dicha probabilidad.\n",
    "\n",
    "Es también llamativo que ninguna de las cuentas que ha contratado un préstamo ha tenido activo las órdenes de Leasing (scatter plot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.TreeExplainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap.force_plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=10\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][i,:], feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[i][['Num_Type_VYBER','Num_Op_Remittances','Ord_Leasing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.expected_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.TreeExplainer(clf_tree_gs_f1).shap_interaction_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La obs 1 y 52 dan valor 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para interpretar Shap Values\n",
    "#http://www.f1-predictor.com/model-interpretability-with-shap/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[0][['Num_Type_VYBER','Num_Op_Remittances','Ord_Leasing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a probar con TreeInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install treeinterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, bias, contributions = ti.predict(clf_tree_gs_f1, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, bias, contributions = ti.predict(clf_tree_gs_f1, X_test)\n",
    "print (\"Prediction\", prediction)\n",
    "print (\"Bias (trainset prior)\", bias)\n",
    "print (\"Feature contributions:\")\n",
    "for c, feature in zip(contributions[0], \n",
    "                             X_test.columns):\n",
    "    print (feature, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert(np.allclose(prediction, bias + np.sum(contributions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert(np.allclose(clf_tree_gs_f1.predict(X_test), bias + np.sum(contributions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pdp_goals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamos a realizar el análisis ahora con modelos para los que no hay explicabilidad directa. Voy a probar con Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Vamos a definir la parrilla para realizar Randomized Grid Search\n",
    "\n",
    "# Máximo número de niveles en el árbol. Damos una distribución con mayor probabilidad en valores pequeños\n",
    "max_depth1 = [int(x) for x in np.linspace(2, 20, num = 10)]\n",
    "max_depth2 =[int(x) for x in np.linspace(30, 100, num = 4)]\n",
    "max_depth=max_depth1 + max_depth2\n",
    "#max_depth.append(None)\n",
    "\n",
    "# Mínimo número de observaciones en cada hoja.Damos una distribución con mayor probabilidad en valores pequeños\n",
    "min_samples_leaf_1 = [int(x) for x in np.linspace(5, 50, num = 10)]\n",
    "min_samples_leaf_2 = [int(x) for x in np.linspace(60, 100, num = 5)]\n",
    "min_samples_leaf=min_samples_leaf_1+min_samples_leaf_2\n",
    "#min_samples_leaf.append(None)\n",
    "\n",
    "# Creamos la grid aleatoria\n",
    "random_grid = {'max_depth': max_depth,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               }\n",
    "\n",
    "clf_tree = DecisionTreeClassifier()\n",
    "clf_tree_random_p= RandomizedSearchCV(estimator = clf_tree, param_distributions = random_grid, n_iter = 100, cv = 5,scoring=\"precision\")\n",
    "clf_tree_random_f1= RandomizedSearchCV(estimator = clf_tree, param_distributions = random_grid, n_iter = 100, cv = 5,scoring=\"f1\")\n",
    "\n",
    "#Comenzamos por el DataFrame X\n",
    "#Generamos conjuntos de train y de test. Para el test usamos el 20% de las observaciones\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf_tree_random_p.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_random_p.best_params_)\n",
    "\n",
    "clf_tree_p = clf_tree_random_p.best_estimator_  \n",
    "clf_tree_p.fit(X_train,y_train)\n",
    "\n",
    "eval_modelo_2 (clf_tree_p,X_train,y_train, X_test,y_test)\n",
    "\n",
    "X.columns[clf_tree_p.feature_importances_>0.10]\n",
    "\n",
    "#Optimizamos ahora el F1 Score\n",
    "\n",
    "clf_tree_random_f1.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",clf_tree_random_f1.best_params_)\n",
    "\n",
    "clf_tree_f1 = clf_tree_random_f1.best_estimator_  \n",
    "clf_tree_f1.fit(X_train,y_train)\n",
    "\n",
    "eval_modelo_2 (clf_tree_f1,X_train,y_train, X_test,y_test)\n",
    "\n",
    "X.columns[clf_tree_f1.feature_importances_>0.10]\n",
    "\n",
    "#Parece que los modelos anteriores overfittean. Vamos a probar un modelo más sencillo\n",
    "clf_tree = DecisionTreeClassifier(min_samples_leaf=20,max_depth=3)\n",
    "clf_tree.fit(X_train,y_train)\n",
    "eval_modelo_2(clf_tree,X_train,y_train, X_test,y_test)\n",
    "\n",
    "X.columns[clf_tree_f1.feature_importances_>0.10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
